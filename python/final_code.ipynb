{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f1b4916-fdef-4dde-9ea9-7b1a99cdd588",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7db3b4df-9291-4152-a64f-9c6ab87a0782",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'job_postings.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# imports\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m job_postings \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mjob_postings.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpython\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m companies \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompanies.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m company_industries \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompany_industries.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1024\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1011\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1012\u001b[0m     dialect,\n\u001b[1;32m   1013\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1020\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1021\u001b[0m )\n\u001b[1;32m   1022\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1024\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:618\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    615\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    617\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 618\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1618\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1617\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1618\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1878\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1876\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1877\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1878\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1880\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1889\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'job_postings.csv'"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "job_postings = pd.read_csv('job_postings.csv', engine=\"python\")\n",
    "companies = pd.read_csv('companies.csv')\n",
    "company_industries = pd.read_csv('company_industries.csv')\n",
    "employee_counts = pd.read_csv('employee_counts.csv')\n",
    "industries = pd.read_csv('industries.csv')\n",
    "job_industries = pd.read_csv('job_industries.csv')\n",
    "job_skills = pd.read_csv('job_skills.csv')\n",
    "salaries = pd.read_csv('salaries.csv')\n",
    "skills = pd.read_csv('skills.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e18afeb-3a41-4e44-bb51-4a015f37e1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge datasets\n",
    "jobs = pd.merge(job_postings, job_industries, how='left', on='job_id')\n",
    "#print(\"merge w job_postings and job_industries: \", jobs.shape[0])\n",
    "jobs = jobs.merge(industries, how='left', on='industry_id')\n",
    "#print(\"merge w industries: \", jobs.shape[0])\n",
    "jobs = jobs.merge(job_skills, how='left', on='job_id')\n",
    "#print(\"merge w job_skills: \", jobs.shape[0])\n",
    "jobs = jobs.merge(skills, how='left', on='skill_abr')\n",
    "#print(\"merge w skills: \", jobs.shape[0])\n",
    "jobs = jobs.merge(salaries, how='left', on='job_id')\n",
    "#print(\"merge w salaries: \", jobs.shape[0])\n",
    "jobs = jobs.merge(companies, how='left', on='company_id')\n",
    "#print(\"merge w companies: \", jobs.shape[0])\n",
    "jobs = jobs.merge(company_industries, how='left', on='company_id')\n",
    "#print(\"merge w company_industries: \", jobs.shape[0])\n",
    "jobs = jobs.merge(employee_counts, how='left', on='company_id')\n",
    "#print(\"merge w employee_counts: \", jobs.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf6fb85-6973-4c6d-bfda-4415696b0b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping duplicate columns\n",
    "jobs.drop(columns=['max_salary_y', 'med_salary_y', 'min_salary_y',\n",
    "                   'pay_period_y', 'currency_y', 'compensation_type_y'], inplace=True)\n",
    "# dropping irrelevant columns\n",
    "jobs.drop(columns=['scraped', 'salary_id', 'url', 'time_recorded', 'currency_x', 'posting_domain',\n",
    "                   'industry_id', 'closed_time', 'listed_time'], inplace=True)\n",
    "# renaming columns\n",
    "jobs.rename(columns={'max_salary_x':'max_salary', 'med_salary_x':'med_salary', 'min_salary_x':'min_salary',\n",
    "                     'compensation_type_x':'compensation_type', 'description_x':'job_description',\n",
    "                     'state':'company_state', 'country':'company_country', 'city':'company_city',\n",
    "                     'zip_code':'company_zip_code', 'address':'company_address', 'name':'company_name',\n",
    "                     'industry_name':'job_industry/function', 'industry':'company_industry',\n",
    "                     'description_y':'company_description', 'pay_period_x':'pay_period'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "352569d3-ddad-4680-9fa2-bc1f1517fd0d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'job_postings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m job_postings_reduce_dim \u001b[38;5;241m=\u001b[39m \u001b[43mjob_postings\u001b[49m\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscraped\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompensation_type\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mskills_desc\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapplication_url\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpay_period\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjob_posting_url\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#job_id(to merge), company_id(to merge), title, description, max_salary, med_salary, min_salary, formatted_work_type, location, applies, original_listed_time,\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#remote_allowed,\u001b[39;00m\n\u001b[1;32m      4\u001b[0m job_postings_reduce_dim\u001b[38;5;241m.\u001b[39mcolumns\n",
      "\u001b[0;31mNameError\u001b[0m: name 'job_postings' is not defined"
     ]
    }
   ],
   "source": [
    "job_postings_reduce_dim = job_postings.drop(['scraped', 'compensation_type', 'skills_desc', 'application_url', 'pay_period', 'job_posting_url'], axis=1)\n",
    "#job_id(to merge), company_id(to merge), title, description, max_salary, med_salary, min_salary, formatted_work_type, location, applies, original_listed_time,\n",
    "#remote_allowed,\n",
    "job_postings_reduce_dim.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622a89c7-ee63-4000-86fe-d56eccedc73d",
   "metadata": {},
   "source": [
    "### Overall plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71c9f5a-0fbd-4f3f-a1f8-b1e9a9ada46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs['location'] = jobs['location'].replace('New York City Metropolitan Area', 'New York, NY')\n",
    "\n",
    "job_postings_locations = jobs['location'].value_counts().nlargest(21)\n",
    "\n",
    "# Exclude entries where 'location' is \"United States\" or \"0\"\n",
    "job_filtered = job_postings_locations\n",
    "job_filtered = jobs[(jobs['location'] != \"United States\") & (jobs['location'] != \"0\")]\n",
    "job_filtered['location'] = job_filtered['location'].replace('New York City Metropolitan Area', 'New York, NY')\n",
    "\n",
    "# Continue with the analysis after excluding these entries:\n",
    "top_cities_filtered = job_filtered['location'].value_counts().nlargest(10).index.tolist()\n",
    "top_skills_filtered = job_filtered['skill_name'].value_counts().nlargest(5).index.tolist()\n",
    "\n",
    "# Prepare the Data: Filter the dataframe for these top cities and skills, and count job postings\n",
    "filtered_df_filtered = job_filtered[job_filtered['location'].isin(top_cities_filtered) & job_filtered['skill_name'].isin(top_skills_filtered)]\n",
    "aggregated_df_filtered = filtered_df_filtered.groupby(['location', 'skill_name']).size().reset_index(name='job_postings_num')\n",
    "\n",
    "aggregated_df_filtered\n",
    "\n",
    "import altair as alt\n",
    "\n",
    "chart = alt.Chart(aggregated_df_filtered).mark_bar().encode(\n",
    "    x=alt.X('location', axis=alt.Axis(title='Location'), sort = alt.EncodingSortField(field = 'job_postings_num', op='sum', order = 'descending')),\n",
    "    y=alt.Y('job_postings_num:Q', axis=alt.Axis(title='Number of Job Postings')),\n",
    "    color='skill_name:N',\n",
    "    tooltip=['location', 'skill_name', 'job_postings_num']\n",
    ").properties(\n",
    "    title='Job Postings by City, State and Skill',\n",
    "    width=600,\n",
    "    height=400\n",
    ").interactive()\n",
    "\n",
    "chart\n",
    "chart.save(\"overall.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6da8ac2-c1b4-4c84-baf7-c16c5f99a654",
   "metadata": {},
   "source": [
    "### Word cloud plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0bd3df-c149-4189-a035-6c7f9f39588f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract job titles column\n",
    "job_titles = jobs['title']\n",
    "\n",
    "# Join all job titles into a single string\n",
    "all_titles_text = ' '.join(job_titles)\n",
    "\n",
    "# Generate word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_titles_text)\n",
    "\n",
    "# Display the word cloud\n",
    "plt.figure(figsize=(20, 12))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e538bc5-6b97-4d89-8572-ac306d47824a",
   "metadata": {},
   "source": [
    "### Heatmap plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1538f316-7c41-4549-ab83-4a044e8da142",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Assuming 'skills' and 'experience_level' are the relevant columns in your DataFrame\n",
    "# Group by 'experience_level' and 'skills', then count the frequency\n",
    "aggregated_df = jobs.groupby(['experience_level', 'skill_name']).size().reset_index(name='frequency')\n",
    "\n",
    "# Pivot the data to get it in the right format for the heatmap\n",
    "pivot_df = aggregated_df.pivot(index='experience_level', columns='skill_name', values='frequency')\n",
    "\n",
    "# Select top 10 skills\n",
    "top_skills = pivot_df.sum().nlargest(10).index\n",
    "pivot_df = pivot_df[top_skills]\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(pivot_df, cmap='Reds', linewidths=.5)\n",
    "plt.title('Heatmap of Skill Name by Experience Level')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2a2654-ac76-4b41-94d0-2ab27a2cdf21",
   "metadata": {},
   "source": [
    "### Median and Average Salary Map plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03aaa137-3836-4494-91cf-cce057a07f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_state(location):\n",
    "    parts = location.split(',')\n",
    "    if len(parts) > 1:\n",
    "        state = parts[-1].strip()\n",
    "        if state != 'United States':\n",
    "            return state\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3ed395-24d0-45d6-96e8-0e758b9b53f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_postings_reduce_dim['state'] = job_postings_reduce_dim['location'].apply(extract_state)\n",
    "\n",
    "job_postings_reduce_dim = job_postings_reduce_dim.dropna(subset=['state'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d43d03-b620-49bd-bb1b-141baeabd582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary mapping state abbreviations to full state names\n",
    "state_abbr_to_name = {\n",
    "    'AL': 'Alabama',\n",
    "    'AK': 'Alaska',\n",
    "    'AZ': 'Arizona',\n",
    "    'AR': 'Arkansas',\n",
    "    'CA': 'California',\n",
    "    'CO': 'Colorado',\n",
    "    'CT': 'Connecticut',\n",
    "    'DE': 'Delaware',\n",
    "    'FL': 'Florida',\n",
    "    'GA': 'Georgia',\n",
    "    'HI': 'Hawaii',\n",
    "    'ID': 'Idaho',\n",
    "    'IL': 'Illinois',\n",
    "    'IN': 'Indiana',\n",
    "    'IA': 'Iowa',\n",
    "    'KS': 'Kansas',\n",
    "    'KY': 'Kentucky',\n",
    "    'LA': 'Louisiana',\n",
    "    'ME': 'Maine',\n",
    "    'MD': 'Maryland',\n",
    "    'MA': 'Massachusetts',\n",
    "    'MI': 'Michigan',\n",
    "    'MN': 'Minnesota',\n",
    "    'MS': 'Mississippi',\n",
    "    'MO': 'Missouri',\n",
    "    'MT': 'Montana',\n",
    "    'NE': 'Nebraska',\n",
    "    'NV': 'Nevada',\n",
    "    'NH': 'New Hampshire',\n",
    "    'NJ': 'New Jersey',\n",
    "    'NM': 'New Mexico',\n",
    "    'NY': 'New York',\n",
    "    'NC': 'North Carolina',\n",
    "    'ND': 'North Dakota',\n",
    "    'OH': 'Ohio',\n",
    "    'OK': 'Oklahoma',\n",
    "    'OR': 'Oregon',\n",
    "    'PA': 'Pennsylvania',\n",
    "    'RI': 'Rhode Island',\n",
    "    'SC': 'South Carolina',\n",
    "    'SD': 'South Dakota',\n",
    "    'TN': 'Tennessee',\n",
    "    'TX': 'Texas',\n",
    "    'UT': 'Utah',\n",
    "    'VT': 'Vermont',\n",
    "    'VA': 'Virginia',\n",
    "    'WA': 'Washington',\n",
    "    'WV': 'West Virginia',\n",
    "    'WI': 'Wisconsin',\n",
    "    'WY': 'Wyoming'\n",
    "}\n",
    "\n",
    "# Replace state abbreviations with full state names\n",
    "job_postings_reduce_dim['state'] = job_postings_reduce_dim['state'].map(state_abbr_to_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4210f25b-fcfe-4f11-8815-74e501f7054d",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_postings_reduce_dim = job_postings_reduce_dim.dropna(subset=['med_salary'], how='any')\n",
    "\n",
    "minimum_salary = job_postings_reduce_dim['med_salary'].min()\n",
    "print(\"Minimum salary:\", minimum_salary)\n",
    "minimum_salary = job_postings_reduce_dim['med_salary'].median()\n",
    "print(\"Median salary:\", minimum_salary)\n",
    "minimum_salary = job_postings_reduce_dim['med_salary'].max()\n",
    "print(\"Maximum salary:\", minimum_salary)\n",
    "\n",
    "# Find the top 10 most frequent industries\n",
    "top_industries = job_postings_reduce_dim['title'].value_counts().head(10).index.tolist()\n",
    "\n",
    "# Create a DataFrame containing only the top industries\n",
    "top_industries_df = job_postings_reduce_dim[job_postings_reduce_dim['title'].isin(top_industries)].copy()\n",
    "\n",
    "num_rows = top_industries_df.shape[0]\n",
    "print(\"Number of rows in the DataFrame:\", num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3355e77b-528f-4ae6-adcc-40ea573ccc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import folium\n",
    "from branca.colormap import LinearColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ad5ccb-4d94-47cd-90f3-f905b31fe97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_data = gpd.read_file(\"C:\\\\Users\\\\hjlac\\\\Downloads\\\\ne_110m_admin_1_states_provinces\\\\ne_110m_admin_1_states_provinces.shx\")\n",
    "print(state_data.columns)\n",
    "color_maps = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa93fca6-d1fb-473a-886b-70ce5c7a820f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(state_data['name'])\n",
    "\n",
    "print(\"\\njob_postings_reduce_dim 'state' column:\")\n",
    "print(job_postings_reduce_dim['state'].iloc[0])  # Assuming you have a DataFrame named 'job_postings_reduce_dim'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf99da0-7376-4d24-9ba9-801944a8524d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique state names in GeoJSON data:\", state_data['name'].unique())\n",
    "print(\"Unique state names in DataFrame:\", job_postings_reduce_dim['state'].unique())\n",
    "state_data = state_data[state_data['name'] != 'District of Columbia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9e9f12-22c4-4f04-98de-d37d3cda5b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique state names from GeoJSON data\n",
    "geojson_states = state_data['name'].unique()\n",
    "\n",
    "# Get unique state names from DataFrame\n",
    "data_states = job_postings_reduce_dim['state'].dropna().unique()\n",
    "\n",
    "# Convert both lists to sets for easier comparison\n",
    "geojson_states_set = set(geojson_states)\n",
    "data_states_set = set(data_states)\n",
    "\n",
    "# Check for discrepancies\n",
    "discrepancies = geojson_states_set.symmetric_difference(data_states_set)\n",
    "\n",
    "# Print discrepancies\n",
    "print(\"Discrepancies:\", discrepancies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c8a43d-d86a-4fd8-a05e-7cabbca837ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from branca.colormap import LinearColormap\n",
    "\n",
    "# Merge state_data_filtered with job_postings_reduce_dim on state name\n",
    "merged_data = state_data.merge(job_postings_reduce_dim, how='inner', left_on='name', right_on='state')\n",
    "\n",
    "# Identify hourly wages under $200\n",
    "hourly_wages_under_200 = merged_data[merged_data['med_salary'] < 1000]\n",
    "\n",
    "# Calculate annualized salaries for hourly wages under $200\n",
    "hours_per_year = 2080  # Assuming 40 hours per week for 52 weeks\n",
    "hourly_wages_under_200['annual_salary'] = hourly_wages_under_200['med_salary'] * hours_per_year\n",
    "\n",
    "# Replace original hourly wages with annualized salaries in the merged DataFrame\n",
    "merged_data.loc[hourly_wages_under_200.index, 'med_salary'] = hourly_wages_under_200['annual_salary']\n",
    "\n",
    "# Calculate average salaries for each state\n",
    "average_salaries = merged_data.groupby('name')['med_salary'].mean().reset_index()\n",
    "average_salaries.columns = ['name', 'average_salary']  # Rename columns for clarity\n",
    "\n",
    "# Merge average salaries with merged_data\n",
    "merged_data = merged_data.merge(average_salaries, on='name')\n",
    "\n",
    "# Create a Folium map object\n",
    "m = folium.Map(location=[31.9686, -99.9018], zoom_start=6)\n",
    "\n",
    "# Define colormap for median salary\n",
    "median_salary_cmap = LinearColormap(['magenta', 'cyan', 'blue'], vmin=24000, vmax=140000)\n",
    "\n",
    "# Create GeoJson layer for median salary\n",
    "median_salary_layer = folium.GeoJson(\n",
    "    merged_data,\n",
    "    name='Median Salary ($)',\n",
    "    style_function=lambda x: {\n",
    "        'fillColor': median_salary_cmap(x['properties']['med_salary']),\n",
    "        'color': 'black',\n",
    "        'weight': 1,\n",
    "        'fillOpacity': 0.2,  # Adjust transparency here\n",
    "    },\n",
    "    tooltip=folium.GeoJsonTooltip(fields=['name', 'med_salary'], aliases=['State', 'Median Salary'], labels=True, sticky=False),\n",
    ")\n",
    "\n",
    "# Define colormap for average salary\n",
    "average_salary_cmap = LinearColormap(['red', 'yellow', 'green'], vmin=average_salaries['average_salary'].min(), vmax=average_salaries['average_salary'].max())\n",
    "\n",
    "# Create GeoJson layer for average salary\n",
    "average_salary_layer = folium.GeoJson(\n",
    "    merged_data,\n",
    "    name='Average Salary ($)',\n",
    "    style_function=lambda x: {\n",
    "        'fillColor': average_salary_cmap(x['properties']['average_salary']),\n",
    "        'color': 'black',\n",
    "        'weight': 1,\n",
    "        'fillOpacity': 0.05,  # Adjust transparency here\n",
    "    },\n",
    "    tooltip=folium.GeoJsonTooltip(fields=['name', 'average_salary'], aliases=['State', 'Average Salary'], labels=True, sticky=False),\n",
    ")\n",
    "\n",
    "# Add GeoJson layers to the map\n",
    "median_salary_layer.add_to(m)\n",
    "average_salary_layer.add_to(m)\n",
    "\n",
    "# Add colormaps to the map\n",
    "median_salary_cmap.caption = 'Median Salary ($)'\n",
    "average_salary_cmap.caption = 'Average Salary ($)'\n",
    "median_salary_cmap.add_to(m)\n",
    "average_salary_cmap.add_to(m)\n",
    "\n",
    "# Add layer control to toggle between layers\n",
    "folium.LayerControl(collapsed=False).add_to(m)\n",
    "\n",
    "# Title the map\n",
    "title_html = '<h3 align=\"center\" style=\"font-size:20px\"><b>Median and Average Salaries by State</b></h3>'\n",
    "m.get_root().html.add_child(folium.Element(title_html))\n",
    "\n",
    "# Save the map as an HTML file\n",
    "# m.save('interactive_map_median_salary.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
